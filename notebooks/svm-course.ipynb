{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Máquinas-de-Vector-Soporte-(SVM,-Support-Vector-Machine)\" data-toc-modified-id=\"Máquinas-de-Vector-Soporte-(SVM,-Support-Vector-Machine)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Máquinas de Vector Soporte (SVM, Support Vector Machine)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Historia\" data-toc-modified-id=\"Historia-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Historia</a></span></li><li><span><a href=\"#Motivación\" data-toc-modified-id=\"Motivación-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Motivación</a></span></li><li><span><a href=\"#Definición\" data-toc-modified-id=\"Definición-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Definición</a></span><ul class=\"toc-item\"><li><span><a href=\"#kernel\" data-toc-modified-id=\"kernel-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>kernel</a></span><ul class=\"toc-item\"><li><span><a href=\"#Construcción-de-un-kernel\" data-toc-modified-id=\"Construcción-de-un-kernel-1.3.1.1\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>Construcción de un kernel</a></span></li><li><span><a href=\"#Ejemplos-de-funciones-kernel-más-utilizadas\" data-toc-modified-id=\"Ejemplos-de-funciones-kernel-más-utilizadas-1.3.1.2\"><span class=\"toc-item-num\">1.3.1.2&nbsp;&nbsp;</span>Ejemplos de funciones kernel más utilizadas</a></span></li></ul></li></ul></li><li><span><a href=\"#Aplicaciones\" data-toc-modified-id=\"Aplicaciones-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Aplicaciones</a></span></li><li><span><a href=\"#SVM-lineales\" data-toc-modified-id=\"SVM-lineales-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>SVM lineales</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM-con-márgen-máximo-en-el-espacio-de-características\" data-toc-modified-id=\"SVM-con-márgen-máximo-en-el-espacio-de-características-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>SVM con márgen máximo en el espacio de características</a></span></li><li><span><a href=\"#SVM-con-margen-blando\" data-toc-modified-id=\"SVM-con-margen-blando-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>SVM con margen blando</a></span></li><li><span><a href=\"#Ventajas-de-las-SVM\" data-toc-modified-id=\"Ventajas-de-las-SVM-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Ventajas de las SVM</a></span></li><li><span><a href=\"#Inconvenientes-de-las-SVM\" data-toc-modified-id=\"Inconvenientes-de-las-SVM-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Inconvenientes de las SVM</a></span></li></ul></li><li><span><a href=\"#Validación-de-los-modelos-SVM\" data-toc-modified-id=\"Validación-de-los-modelos-SVM-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Validación de los modelos SVM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Validación-cruzada-o-leave-one-out\" data-toc-modified-id=\"Validación-cruzada-o-leave-one-out-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Validación cruzada o <em>leave-one-out</em></a></span><ul class=\"toc-item\"><li><span><a href=\"#Error-de-la-validación-cruzada\" data-toc-modified-id=\"Error-de-la-validación-cruzada-1.6.1.1\"><span class=\"toc-item-num\">1.6.1.1&nbsp;&nbsp;</span>Error de la validación cruzada</a></span></li></ul></li><li><span><a href=\"#Validación-cruzada-k-fold\" data-toc-modified-id=\"Validación-cruzada-k-fold-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Validación cruzada <em>k-fold</em></a></span><ul class=\"toc-item\"><li><span><a href=\"#Error-de-la-validación-cruzada-de-K-iteraciones\" data-toc-modified-id=\"Error-de-la-validación-cruzada-de-K-iteraciones-1.6.2.1\"><span class=\"toc-item-num\">1.6.2.1&nbsp;&nbsp;</span>Error de la validación cruzada de K iteraciones</a></span></li></ul></li><li><span><a href=\"#Validación-cruzada-aleatoria\" data-toc-modified-id=\"Validación-cruzada-aleatoria-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>Validación cruzada aleatoria</a></span><ul class=\"toc-item\"><li><span><a href=\"#Error-de-la-validación-cruzada-aleatoria\" data-toc-modified-id=\"Error-de-la-validación-cruzada-aleatoria-1.6.3.1\"><span class=\"toc-item-num\">1.6.3.1&nbsp;&nbsp;</span>Error de la validación cruzada aleatoria</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Construcción-de-una-Máquina-de-Vector-Soporte-con-R\" data-toc-modified-id=\"Construcción-de-una-Máquina-de-Vector-Soporte-con-R-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Construcción de una Máquina de Vector Soporte con R</a></span><ul class=\"toc-item\"><li><span><a href=\"#Support-Vector-Machine-Learning\" data-toc-modified-id=\"Support-Vector-Machine-Learning-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Support Vector Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Support-Vector-Machine\" data-toc-modified-id=\"Linear-Support-Vector-Machine-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Linear Support Vector Machine</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"./figures/Logo_OWL_Metabolomics.png\" border=0 width=\"70%\"> \n",
    "<div style=\"text-align:center;\">\n",
    "<font size=7>Máquinas de Vector Soporte con R</font>\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align:right;\">\n",
    "imartinez[at]owlmetabolomics.com  \n",
    "calonso[at]owlmetabolomics.com\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size=2 color=\"#606060\" align=\"right\"><i>The secret language of statistics, so appealing in a fact-minded culture, is employed to sensationalize, inflate, confuse,\n",
    "and oversimplify. Statistical methods and statistical terms are necessary in reporting the mass data of social and\n",
    "economic trends, business conditions, opinion polls, the census. But without writers who use the words with honesty and\n",
    "understanding and readers who know what they mean, the result can only be semantic nonsense.</i>\n",
    "<br>\n",
    "<b>How to lie with statistics</b>, 1954</font>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máquinas de Vector Soporte (SVM, Support Vector Machine)\n",
    "\n",
    "En el contexto de Machine Learning, las Máquinas de Vector Soporte son modelos de aprendizaje supervisado asociados a los algoritmos que analizan datos para su análisis de clasificación y/o regresión. Dado un conjunto de muestras de entrenamiento, cada una de ellas clasificada en una categoría, el algoritmo de entrenamiento de una SVM construye un modelo que asigna una clase a cada observación. Un modelo SVM es una representación de estas muestras en el espacio de tal manera que las muestras de cada categoría están separadas de forma clara.\n",
    "\n",
    "## Historia\n",
    "\n",
    "El algoritmo original de las Máquinas de Vector Soporte fue escrito por *Vladimir N. Vapnik* y *Alexey Ya. Chervonenkis* en 1963. En 1992, *Bernhard E. Boser*, *Isabelle M. Guyon* y *Vladimir N. Vapnik* sugirieron una metodología para crear clasificadores no lineales aplicando el mismo concepto de hiperplanos de margen máximo.\n",
    "\n",
    "## Motivación\n",
    "\n",
    "La clasificación de muestras es una tarea común en Machine learning en la que dados unos datos en los que cada observación pertenece a alguna clase y la finalidad es decidir a qué clase asignar una nueva observación. En el caso de las SVM, cada observación se considera un vector de *p* dimensiones (tenemos *p* variables) y tratamos de separar cada clase. Si lo hacemos con hiperplano de dimensiones *p*-1 estaremos aplicando un clasificador lineal. Hay muchos hiperplanos que podrían clasificar nuestros datos. Podemos razonar y buscar aquel hiperplano que muestra la mayor separación entre clases. Elegimos entonces aquel hiperplano cuya distancia a los puntos más cercanos de cada lado sea máxima. Si tal hiperplano existe se denomina hiperplano de máximo margen y el clasificador lineal asociado se define como clasificador de máximo margen.\n",
    "\n",
    "## Definición\n",
    "\n",
    "Podemos definir más formalmente este hiperplano. Una Máquina de Vector Soporte construye un hiperplano o conjunto de hiperplanos n-dimensionales que pueden ser usado para clasificación, regresión u otras tareas. Geométricamente, una buena separación se alcanzará por aquel hiperplano que tenga la mayor distancia entre las observaciones de cada clase en las muestras de entrenamiento.\n",
    "\n",
    "### kernel\n",
    "\n",
    "A veces, el problema original puede ser resuelto en un espacio de dimensión finita, pero en otras ocasiones sucede que los conjuntos a discriminar no tienen una separación lineal en ese espacio. Para solventar este inconveniente, el espacio de dimensión finita donde está planteado el problema puede ser transformado a un espacio de dimensión mayor, donde es esperable que la separación entre clases se más fácil de calcular. \n",
    "\n",
    "El aumentar la dimensión del espacio en el que estamos trabajando implica un coste computacional mayor. Para que este aumento sea razonable las transformaciones a espacios de dimensión mayor se diseñan de tal manera que los productos escalares en estos nuevos espacios puedan ser calculados fácilmente en términos de las variables iniciales. Para ello se utilizan las funciones kernel $k(x,y)$ seleccionadas específicamente para resolver este problema. Los hiperplanos en una mayor dimensión son definidos como aquellos conjuntos de puntos tales que su producto escalar con un vector en ese espacio es constante. Los vectores que definen los hiperplanos pueden ser elegidos como una combinación\n",
    "lineal con parámetros $\\alpha_i$ de imágenes de los vectores de características $x_i$. Si elegimos el hiperplano con estas propiedades, los puntos $x$ en el espacio de características son llevados hiperplanos que se define por la siguiente relación:\n",
    "\n",
    "$$\\sum_{i} \\alpha_i \\cdot k\\left(x_i, x \\right) = \\mbox{Constante}$$\n",
    "\n",
    "Tenemos que tener en cuenta que si $k(x,y)$ se vuelve pequeño a medida que $y$ crece más lejos de $x$, cada término de la suma mide el grado de cercanía de la prueba del punto $x$ al punto de base de datos correspondiente $x_i$. De esta manera, la suma de los núcleos anteriores puede usarse para medir la proximidad relativa de cada punto de prueba a los puntos de datos originados en uno u otro de los conjuntos a discriminar.\n",
    "\n",
    "#### Construcción de un kernel\n",
    "\n",
    "Como hemos comentado, la función kernel nos lleva el espacio de características (donde están nuestros datos) a un nuevo espacio de dimensión mayor de tal manera que sea fácil calcular el producto escalar. Sea el kernel definido como\n",
    "\n",
    "$$\\phi\\left( x_i, x_2\\right) = \\phi\\left(x_1^2, x_2^2, \\sqrt{x_1, x_2} \\right) = \\left(z_1, z_2, z_3\\right)$$\n",
    "\n",
    "Que lleva un punto $x\\in R^2$ a $z \\in R^3$. En la siguiente figura observamos que este kernel separa el interior de la circunferencia con el exterior con un hiperplano en $R^3$, siendo mucho más fácil separar las clases en las que están divididas las observaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\int f(x)\\,\\text{d}x = F(x)\n",
    "\\label{eq1}\n",
    "\\tag{eq1text}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación \\ref{eq1} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las Máquinas de Vector Soporte funcionan agrupando los puntos de las características según sus clases. En la figura 1.2 se generan dos vectores de características  bidimensionales $x = \\left(x_1, x_2\\right)$ de tal manera que la clase $y = −1$ puntos (triángulos) están bien separados de la clase $y = 1$ (círculos).\n",
    "\n",
    "El algoritmo encuentra el mayor margen lineal posible que separa estas dos regiones. Los separadores se apoyan sobre los puntos avanzados que están justo en la línea frente a sus respectivas regiones. Estos puntos, marcados como dos triángulos en negrita y un círculo en negrita en la figura 1.2, se llaman los vectores de apoyo o vectores soporte, ya que están apoyando las líneas de separación. De hecho, la tarea de aprendizaje del algoritmo de Máquinas de Vector Soporte consiste en determinar estos puntos vector de soporte y la distancia de margen que separa las regiones. Después del entrenamiento, todos los demás puntos de no apoyo no se usará para futuras predicciones.\n",
    "\n",
    "En el espacio de características lineales, los vectores soporte se suman a un vector de hipótesis general $h$,\n",
    "\n",
    "$$h = \\sum_{i} c_i\\cdot x_i$$\n",
    "\n",
    "De modo que las fronteras de clasificación están dadas por las líneas $hx+b = 1$ y $hx + b = −1$ centradas alrededor de $hx + b = 0$.\n",
    "\n",
    "El código 4.2 en el anexo es una modificación de la implementación de la función `ksvm()` en el paquete [kernlab](https://cran.r-project.org/web/packages/kernlab/index.html) de R , haciendo uso de los tutoriales de Jean-Philippe Vert para representar las líneas de separación de clasificación mediante un kernel lineal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la figura 1.3 se ilustra un ejemplo en el que las observaciones no están separados. Los puntos de la clase $y = 1$ (círculos) se colocan en una región interior rodeada por puntos de clase $y = −1$ (triángulos). En este ejemplo no hay una sola línea recta (lineal) que pueda separar ambas regiones. Sin embargo es posible encontrar un separador lineal mediante la transformación de los puntos $x = \\left\\lbrace x_1, x_2\\right\\rbrace$ del espacio de características a un espacio cuadrático de núcleos con puntos dados por las correspondientes coordenadas cuadradas $\\left\\lbrace x_1^2, x_2^2\\right\\rbrace$. El código en R puede consultarse en el código 4.3 en el anexo. \n",
    "\n",
    "La técnica de transformar el espacio de características en una medida que permite una separación lineal puede formalizarse en términos de kernel. Suponiendo que $\\Phi()$ sea una función de transformación vectorial de coordenadas, un espacio de coordenadas cuadráticas sería $\\left\\lbrace\\phi(x_1), \\Phi(x_2)\\right\\rbrace$ = $\\left\\lbrace x_1^2, x_2^2\\right\\rbrace$. La búsqueda de separación de la SVM está actuando ahora en el espacio transformado para encontrar los vectores de soporte que generan la condición:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h\\cdot\\Phi(x) + b = \\pm 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el vector de hipótesis $h$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h = \\sum_{i} c_{i} \\cdot \\Phi (x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada por la suma sobre los puntos vectoriales de soporte $x_i$, poniendo ambas expresiones juntas obtenemos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i} c_{i}\\cdot K(x_i, x) + b = \\pm 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función de kernel escalar $K(x_i, x) = \\Phi(x_i)\\cdot \\Phi(x)$. El kernel se compone del producto escalar entre un vector soporte $x_i$ y otro punto vector $x$ de características en el espacio transformado.\n",
    "\n",
    "En la práctica, el algoritmo SVM puede expresarse completamente en términos de kernels sin tener que especificar realmente la transformación de espacio de entidad. Los núcleos populares son, por ejemplo, potencias superiores del producto escalar lineal (kernel polinomial). Otro ejemplo es una probabilidad pesada de distancia entre dos puntos (kernel gaussiano). La implementación de una función de núcleo cuadrático bidimensional permite al algoritmo SVM encontrar vectores soporte y separar correctamente las regiones. \n",
    "\n",
    "En la figura 1.3 se muestra que regiones no lineales se pueden separar linealmente después de una transformación adecuada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos de funciones kernel más utilizadas\n",
    "\n",
    "kernel lineal, el más sencillo de los kernels posibles:\n",
    "\n",
    "$$k(x,x') = \\left< x, x'\\right>$$\n",
    "\n",
    "kernel de base radial (_RBF, Laplace Radial Basis Function_):\n",
    "\n",
    "$$k(x,x') = \\left< e^{-\\sigma\\cdot || x-x' ||}\\right>$$\n",
    "\n",
    "kernel de base radial (_RBF, Laplace radial Basis Function_):\n",
    "\n",
    "$$k(x,x') = \\left< e^{-\\sigma\\cdot || x-x' ||^2}\\right>$$\n",
    "\n",
    "kernel polinomial:\n",
    "\n",
    "$$k(x,x') = \\left<\\beta_1 (x,x') + \\beta_0 \\right>^d$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicaciones\n",
    "\n",
    "Las SVMs pueden ser utilizados para resolver varios problemas del mundo real, como por ejemplo:\n",
    "\n",
    "  * La clasificación de las imágenes se puede realizar usando SVMs. Los resultados experimentales muestran que las SVM logran una precisión de búsqueda significativamente mayor que otros algoritmos de clasificación supervisada.\n",
    "  \n",
    "  * Los caracteres escritos a mano se pueden reconocer usando SVMs. Estos algoritmos son conocidos como algoritmos ocr. Una aplicación muy conocida de reconocimiento de caracteres son los captchas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM lineales\n",
    "\n",
    "Dado un conjunto n de observaciones de entrenamiento de la forma:\n",
    "\n",
    "$$(\\vec{x}_1, y_1), (\\vec{x}_2, y_2), \\ldots, (\\vec{x}_n, y_n)$$\n",
    "\n",
    "Donde $y_i$ toma los valores -1 o 1, indicando la clase a la que pertenece cada punto $\\vec{x}_i$. Cada $\\vec{x}_i$ es un vector de $p$ dimensiones. Queremos encontrar el hiperplano de margen máximo que divide el grupo de puntos $\\vec{x}_i$ entre los que verifican que $y_i = 1$ de conjunto de puntos que verifican $y_i = −1$. Este hiperplano es definido como aquel cuya distancia a los puntos más cercanos $\\vec{x}_i$ de cada clase es máxima.\n",
    "\n",
    "Un hiperplano puede ser descrito como el conjunto de puntos $\\vec{x}$ que satisfacen la siguiente condición:\n",
    "\n",
    "$$\\vec{w}\\cdot \\vec{x} -b = 0$$\n",
    "\n",
    "Donde $\\vec{w}$ es el vector normal (no necesariamente normalizado) al hiperplano.\n",
    "\n",
    "El parámetro\n",
    "\n",
    "$$\\dfrac{b}{||\\vec{w}||}$$\n",
    "\n",
    "determina el desplazamiento de la hiperplano desde el origen a\n",
    "lo largo del vector normal $\\vec{w}$.\n",
    "\n",
    "Para el caso no lineal existen dos casos que vale la penamencionar:\n",
    " \n",
    "  * El primero de estos se presenta cuando los datos pueden serseparables con margen máximo pero en un espacio decaracterísticas (el cual es de una mayor dimensionalidad y seobtiene a través de una trasformación a las variables delespacio de entrada) mediante el uso de una función kernel)\n",
    " \n",
    "  * El segundo caso especial de las SVM denominado _SoftMargin_ o margen blando, es utilizado cuando no es posibleencontrar una trasformación de los datos que permita separarlos linealmente, bien sea en el espacio de entrada o en el espacio de características.\n",
    "  \n",
    "### SVM con márgen máximo en el espacio de características\n",
    "\n",
    "Hay casos donde los datos no pueden ser separados linealmente através de un hiperplano óptimo en el espacio de entrada. En muchas situaciones, los datos, a través de una transformación no lineal del espacio de entradas, pueden ser separados linealmente pero en un espacio de características y se pueden aplicar los mismos razonamientos que para las SVM lineales con margen máximo.La trasformación de los datos de un espacio inicial a otro de mayor dimensión se logra mediante el uso de la función kernel. Una función núcleo o kernel es un producto interno en el espaciode características, que tiene su equivalente en el espacio de entrada.\n",
    "\n",
    "$$k(x,x') = \\left<\\Phi(x), \\Phi(x')\\right>$$\n",
    "\n",
    "donde _k_, es una función simétrica positiva definida que cumple las condiciones de Mercer.\n",
    "\n",
    "Entre los kernels más comunes, se encuentran: la función lineal, polinomial, RBF (Radial Basis Function), ERBF (Exponential Radial Basis Function), entre otros.\n",
    "\n",
    "El problema de optimización a resolver para las WSVM con margen blando está definido por un modelo de programación cuadrática con restricciones, es decir:\n",
    "\n",
    "$${\\begin{cases} \\mathrm {\\mbox{maximizar:}} \\sum_{i} \\alpha_i - \\dfrac{1}{2}\\sum_{i,j} y_iy_j\\alpha_i\\alpha_j k\\left(x_i, x_j\\right),\\\\\\\\ \\mathrm {\\mbox{sujeto a:}} \\sum_{i}y_i\\alpha_i = 0 \\mathrm{\\mbox{ y }} 0 \\le\\alpha_i \\le 1, i\\in 1, 2, ..., N \\end{cases}}$$\n",
    "\n",
    "Este problema de optimización se resuelve introduciendo los multiplicadores de Lagrange, así los datos de entrenamiento sólo aparecerán en forma de una combinación de vectores y la resolución del problema, se puede hallar resolviendo el problema dual dado por las ecuaciones que preceden.\n",
    "\n",
    "### SVM con margen blando\n",
    "\n",
    "Este tipo particular de las SVM trata aquellos casos donde existen datos de entrada erróneos, ruido o un alto solapamiento de las clases en los datos de entrenamiento, donde se puede ver afectado el hiperplano clasificador, por esta razón se cambia un poco la perspectiva y se busca el mejor hiperplano clasificador que pueda tolerar el ruido en los datos de entrenamiento.\n",
    "\n",
    "$${\\begin{cases} \\mathrm {\\mbox{maximizar:}} \\sum_{i} \\alpha_i - \\dfrac{1}{2}\\sum_{i,j} y_iy_j\\alpha_i\\alpha_j k\\left(x_i, x_j\\right),\\\\\\\\ \\mathrm {\\mbox{sujeto a:}} \\sum_{i}y_i\\alpha_i = 0 \\mathrm{\\mbox{ y }} 0 \\le\\alpha_i \\le C, i\\in 1, 2, ..., N \\end{cases}}$$\n",
    "\n",
    "La función a maximizar es la misma que para el caso de margen máximo, a diferencia de la restricción $0 \\le \\alpha_i \\le C$.\n",
    "\n",
    "###  Ventajas de las SVM\n",
    "\n",
    "Las Máquinas de Vectores Soporte tienen ciertas característicasque las han puesto en ventaja respecto a otras técnicas popularesde clasificación y/o regresión.Una de dichas características que vale la pena mencionar es quelas mismas pertenecen a las disciplinas de aprendizaje automáticoo aprendizaje estadístico. La idea que hay detrás de este tipo deaprendizaje es la de hacer que las máquinas puedan iraprendiendo, a través de ejemplos; las salidas correctas paraciertas entradas.La diferencia más notable de las máquinas de vectores de soportecon respecto a otros algoritmos de aprendizaje, es la aplicaciónde un nuevo principio inductivo, que busca la minimización delriesgo estructural, además del uso de una función núcleo o kernel, atribuyéndole una gran capacidad de generalización, incluso cuando el conjunto de entrenamiento es pequeño.\n",
    " \n",
    "Se dice que tanto la capacidad de generalización cómo el proceso de entrenamiento de la máquina no dependen necesariamente del número de atributos, lo que permite un excelente comportamientoen problemas de alta dimensionalidad.\n",
    "\n",
    "### Inconvenientes de las SVM\n",
    "\n",
    "Uno de los más comunes es lo que se conoce como _overtraining_ u _overfitting_ traducido como sobre-entrenamiento, el cual ocurre cuando se han aprendido muy bien los datos de entrenamiento pero no se pueden clasificar bien ejemplos nunca antes vistos (datos de verificación), es decir, una mala generalización del modelo. Otro problema que se puede presentar cuando no se ha aprendido muy bien la característica de los datos de entrenamiento, por lo que se hace una mala clasificación. El experimentador debe tener en cuenta estas consideraciones a la hora de ajustar el modelo ya que de ello depende la exactitud y éxito de la predicción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación de los modelos SVM\n",
    "\n",
    "Como todos los modelos supervisado, las SVM dependen de las observaciones de entrenamiento. Si cambian estas observaciones, los parámetros del modelo pueden cambiar. En las SVM se observa ademas que la construcción del hiper plano depende de los vectores soporte, si se modifican los vectores soporte se modifica el hiperplano, aunque el resto de observaciones sean las mismas.\n",
    "\n",
    "Por este motivo, al generar un modelo SVM se debe analizar su robustez mediante un análisis de validación cruzada. Los análisis de validación nos permite valorar el grado de sobreajuste de nuestro modelo a los datos. Un modelo sobreajustado clasificará muy bien las muestras de entrenamiento, pero su exactitud será muy baja cuando pronostique nuevas observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "Pie de figura",
    "label": "fig:prueba"
   },
   "source": [
    "### Validación cruzada o _leave-one-out_\n",
    "\n",
    "La validación _leave-one-out_ (validación dejando uno fuera) es una validación muy sencilla pero que computacionalmente puede ser muy costosa. El procedimiento de la validación leave-one-out se basa en lo siguiente: Si tenemos n observaciones entonces construimos $n$ Máquinas de Vector Soporte, $\\lbrace SVM_1, SVM_2, ..., SVM_n\\rbrace$, cada una de ellas con $n-1$ muestras, $SVM_i = \\lbrace 1, 2, ..., i − 1, i + 1, ..., n\\rbrace$ y aplicamos el modelo sobre la muestra no utilizada en la construcción del modelo.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/2/2d/Leave-one-out.jpg)\n",
    "\n",
    "#### Error de la validación cruzada\n",
    "\n",
    "En la validación cruzada _leave-one-out_ se realizan tantas iteraciones como muestras (N) tenga el conjunto de datos. De forma que para cada una de las N iteraciones se realiza un cálculo de error. El resultado final lo obtenemos realizando la media aritmética de los N valores de errores obtenidos, según la fórmula:\n",
    "\n",
    "$${\\displaystyle {E}={\\frac {1}{N}}\\sum _{i=1}^{N}E_{i}}$$\n",
    "\n",
    "Donde se realiza el sumatorio de los N valores de error ($E_i$) y se divide entre el valor de N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación cruzada _k-fold_\n",
    "\n",
    "La validación _k-fold_ consiste en separar las muestras en *k* grupos de mismo tamaño. Se construyen entonces *k* Máquinas de Vector Soporte con _k-1_ grupos y se aplica el modelo resultante sobre el grupo de muestras no incluidas en el modelo. Observamos que si _k = n_ estamos ante la validación _leave-one-out_.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/f/f2/K-fold_cross_validation.jpg)\n",
    "\n",
    "#### Error de la validación cruzada de K iteraciones\n",
    "\n",
    "En cada una de las *k* iteraciones de este tipo de validación se realiza un cálculo de error. El resultado final lo obtenemos a partir de realizar la media aritmética de los *k* valores de errores obtenidos, según la fórmula:\n",
    "\n",
    "$${\\displaystyle {E}={\\frac {1}{K}}\\sum _{i=1}^{K}E_{i}}$$\n",
    "\n",
    "Es decir, se realiza el sumatorio de los *k* valores de error y se divide entre el valor de *K*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación cruzada aleatoria\n",
    "\n",
    "Este método consiste al dividir aleatoriamente el conjunto de datos de entrenamiento y el conjunto de datos de prueba. Para cada división la función de aproximación se ajusta a partir de los datos de entrenamiento y calcula los valores de salida para el conjunto de datos de prueba. El resultado final se corresponde a la media aritmética de los valores obtenidos para las diferentes divisiones. La ventaja de este método es que la división de datos entrenamiento-prueba no depende del número de iteraciones. Pero, en cambio, con este método podría haber algunas muestras que quedan sin evaluar y otras que se analicen más de una vez, es decir, los subconjuntos de prueba y entrenamiento se pueden solapar. Este método de validación es interesante hacerlo pero repitiendo muchas veces el proceso, para aseguranos que, efectivamente, todas las muestras son evaluadas en algún momento.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/b/b2/Random_cross_validation.jpg)\n",
    "\n",
    "#### Error de la validación cruzada aleatoria\n",
    "\n",
    "En la validación cruzada aleatoria a diferencia del método anterior, cogemos muestras al azar durante k iteraciones, aunque de igual manera, se realiza un cálculo de error para cada iteración. El resultado final también lo obtenemos a partir de realizar la media aritmética de los K valores de errores obtenidos, según la misma fórmula:\n",
    "\n",
    "$${\\displaystyle {E}={\\frac {1}{K}}\\sum _{i=1}^{K}E_{i}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de una Máquina de Vector Soporte con R\n",
    "\n",
    "Hay muchos paquetes en R que permiten construir SVM como por ejemplo el paquete `e1071` o el paquete `kernlab`. Nosotros vamos a trabajar con el paquete `caret` un paquete diseñado por Max Kuhn precisamente para la búsqueda de\n",
    "modelos, en particular SVM. \n",
    "\n",
    "La práctica que vamos a desarrollar consiste en la creación y validación de una Máquina de Vector Soporte que sea capaz de distinguir el dígito escrito en una imagen de entre los diez posibles dígitos: 0, 1, 2, ..., 8 y 9. Los datos se han generado a partir de 130 imágenes por cada dígito en las que en cada imagen el dígito está escrito con una tipografía diferente. El código para generar dichas imágenes está escrito es el código 4.7 en el anexo.\n",
    "\n",
    "En total tenemos 1300 imágenes de 16x16 píxeles cada una, por lo que tenemos 256 píxeles de información en cada imagen que podemos considerar como 256 variables. Mediante el programa imageMagick hemos pasado cada imagen a información en texto que podemos leer fácilmente con R. En el código 2.1 están descritas las primeras 25 líneas de uno de los ficheros generados.\n",
    "\n",
    "```\n",
    "# ImageMagick pixel enumeration: 16,16,255,srgb\n",
    "0,0: (255,255,255) #FFFFFF white\n",
    "1,0: (255,255,255) #FFFFFF white\n",
    "2,0: (255,255,255) #FFFFFF white\n",
    "3,0: (255,255,255) #FFFFFF white\n",
    "4,0: (255,255,255) #FFFFFF white\n",
    "5,0: (255,255,255) #FFFFFF white\n",
    "6,0: (255,255,255) #FFFFFF white\n",
    "7,0: (255,255,255) #FFFFFF white\n",
    "8,0: (255,255,255) #FFFFFF white\n",
    "9,0: (255,255,255) #FFFFFF white\n",
    "10,0: (255,255,255) #FFFFFF white\n",
    "11,0: (255,255,255) #FFFFFF white\n",
    "12,0: (255,255,255) #FFFFFF white\n",
    "13,0: (255,255,255) #FFFFFF white\n",
    "14,0: (255,255,255) #FFFFFF white\n",
    "15,0: (255,255,255) #FFFFFF white\n",
    "0,1: (255,255,255) #FFFFFF white\n",
    "1,1: (255,255,255) #FFFFFF white\n",
    "2,1: (255,255,255) #FFFFFF white\n",
    "3,1: (255,255,255) #FFFFFF white\n",
    "4,1: (255,255,255) #FFFFFF white\n",
    "5,1: (255,255,255) #FFFFFF white\n",
    "6,1: (255,255,255) #FFFFFF white\n",
    "7,1: (255,255,255) #FFFFFF white\n",
    "```\n",
    "\n",
    "El código anterior son las primeras líneas del fichero en texto de una imagen. La primera fila es la descripción del fichero, la segunda línea nos informa que el punto (0,0) de la imagen es de color blanco. La información está en rgb, hexadecimal y en modo texto (white, black). Nosotros utilizaremos la última columna como la información de nuestras variables.\n",
    "\n",
    "En la figura 2.6 se muestra como ejemplo las 130 imágenes del dígito 8 que utilizaremos para entrenar y validar el modelo de Máquinas de Vector Soporte. Dibujamos el perfil promedio de cada dígito. Las figuras, generadas a 16x16\n",
    "pixeles de resolución nos dan un total de 256 variables en las que tenemos los valores white o black, una variable por cada píxel de la imagen. Nosotros pasaremos estos valores a -1 y 1. Cuanto mayor sea la resolución de la imagen mayor será también el número de variables. Las diferencias claras entre los distintos números, que hace que seamos capaces de distinguirlos, tienen que verse también en el perfil generado por las 256 variables y estas diferencias son las que tiene que encontrar la Máquina de Vector Soporte. En la figura 2.7 se muestran la distribución promedio de cada píxel para cada dígito.\n",
    "\n",
    "## Support Vector Machine Learning\n",
    "\n",
    "Como ya hemos comentado, las máquinas de vector soporte dependen de la función kernel que consideremos. Las más usuales son las funciones kernel lineales y las funciones kernel de base radial. Con los datos de las 1300 imágenes vamos a construir diferentes Máquinas de Vector Soporte utilizando diferentes argumentos y parámetros y compararemos sus resultados.\n",
    "\n",
    "### Linear Support Vector Machine\n",
    "\n",
    "Estimamos la exactitud de una máquina de vector soporte con función kernel lineal. La forma de atacar este problema por parte de la librería `caret` es evaluando cada punto de la malla generada por los diferentes valores de los parámetros sigma ($\\sigma$) y costo ($C$). Una vez evaluados todos los parámetros elegiremos aquél par (sigma, costo) que haya maximizado la exactitud de las predicciones. Valoramos la construcción del modelo considerando 5 repeticiones con el 80% del total de la muestra \n",
    "\n",
    "`trainControl(method = \"cv\", p = 0.8, repeats = 5, search = \"grid\")`\n",
    "\n",
    "Para evaluar un modelo de máquinas de vector soporte con kernel lineal, le pasamos a la función train el argumento `method = \"svmLinear2\"` que será evaluada internamente con las funciones del paquete `kernlab`.\n",
    "\n",
    "En metabolómica, es habitual realizar diferentes transformaciones en los datos para conseguir distribuciones normales o distribuciones tipificadas entre otras muchas. La función train permite considerar diferentes transformaciones sobre los datos. Las más comunes son:\n",
    "\n",
    "  * **Datos originales**: Trabajaremos con los datos originales, sin ningún tipo de preprocesado previo utilizando el comando `preProcess = NULL`. \n",
    "  \n",
    "  * **Datos escalados y centrados**: Si las variables tienen diferentes escalas y variabilidad es interesante considerar el centrado de variables y su escalado, para que no influya en el análisis. Utilizaremos el comando `preProcess = c(\"scale\", \"center\")`.\n",
    "  \n",
    "  $$X' = \\dfrac{X -X'}{\\sigma_{X}}$$\n",
    "  \n",
    "  * **Datos transformados según potencias Box-Cox**: Las transformacion es Box-Cox son una familia de transformaciones cuya finalidad principal es normalizar una variable. El comando que podemos utilizar es `preProcess = \"BoxCox\"`.\n",
    "  \n",
    "  $$y_{i}^{(\\lambda )}={\\begin{cases} \\dfrac{y_{i}^{\\lambda }-1)}{\\lambda} &\\mathrm {si} \\ \\lambda \\neq 0,\\\\\\\\ ln(y_{i})&\\mathrm {si} \\ \\lambda =0\\end{cases}}$$\n",
    "  \n",
    "La figura 2.9 muestra la proyección sobre las dos primeras componentes las observaciones que utilizaremos para entrenar al modelo (65 observaciones de cada dígito). Vemos que hay una separación evidente entre algunos dígitos y otros están juntos. Comprobamos que las observaciones de los dígitos 1 y 7 están cercanas, así como las observaciones de los dígitos 0, 6 y 8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "240px",
    "width": "493px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "254px",
    "left": "45px",
    "right": "1345.53px",
    "top": "582px",
    "width": "280px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
